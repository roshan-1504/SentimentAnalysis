{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcff7e79-0f05-4a78-bc17-3bf8aa5fcefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.5.0)\n",
      "Requirement already satisfied: tensorflow in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.17.0)\n",
      "Requirement already satisfied: absl-py in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras) (13.8.0)\n",
      "Requirement already satisfied: namex in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras) (3.11.0)\n",
      "Requirement already satisfied: optree in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras) (0.12.1)\n",
      "Requirement already satisfied: ml-dtypes in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras) (0.4.0)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras) (24.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (4.25.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (74.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.66.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.17.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install keras tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28ddcaac-7fc6-493f-9904-1ce7fb208dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41e99106-e358-45a9-8ebd-4d095e5811ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Loading the Sentiment140 dataset\n",
    "df = pd.read_csv('../Sentiment140.csv', encoding='ISO-8859-1', header=None)\n",
    "\n",
    "# Assigning column names to the dataset\n",
    "df.columns = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
    "\n",
    "# Remove unnecessary columns\n",
    "df = df[['target', 'text']]\n",
    "\n",
    "# Map the target values (0 = negative, 4 = positive)\n",
    "df['target'] = df['target'].map({0: 0, 4: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bd8e3ed-5d24-42db-8981-34ce606d168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Preprocessing the text data\n",
    "def clean_text(text):\n",
    "    # Removing URLs, mentions, hashtags\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "    \n",
    "    # Remove punctuation and convert to lowercase\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the text cleaning function to the dataset\n",
    "df['text'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce8a893e-fa37-4d34-8c9c-1264e358eb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Splitting the dataset into training and testing sets\n",
    "X = df['text']\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be2904f7-1b15-48d6-81b4-87cc387de97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Tokenizing and padding sequences for CNN input\n",
    "vocab_size = 10000  # Max vocabulary size\n",
    "max_length = 100    # Max sequence length\n",
    "\n",
    "# Tokenizer for converting text to integer sequences\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert the text data to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Padding the sequences to ensure they have the same length\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# One-hot encode the target labels\n",
    "y_train_cat = to_categorical(y_train, num_classes=2)\n",
    "y_test_cat = to_categorical(y_test, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d18b3a1b-e61b-47ad-9618-4852aa9c86fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Building the CNN model\n",
    "embedding_dim = 100  # Dimension of the embedding layer\n",
    "\n",
    "cnn_model = Sequential()\n",
    "\n",
    "# Embedding layer (removed input_length parameter)\n",
    "cnn_model.add(Embedding(vocab_size, embedding_dim))\n",
    "\n",
    "# Convolutional layer\n",
    "cnn_model.add(Conv1D(128, 5, activation='relu'))\n",
    "\n",
    "# Pooling layer\n",
    "cnn_model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# Dense layer\n",
    "cnn_model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "cnn_model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31903080-cbe4-4f25-8e1f-4db868e58826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m18000/18000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m565s\u001b[0m 31ms/step - accuracy: 0.7913 - loss: 0.4412 - val_accuracy: 0.8198 - val_loss: 0.3932\n",
      "Epoch 2/5\n",
      "\u001b[1m18000/18000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m560s\u001b[0m 31ms/step - accuracy: 0.8328 - loss: 0.3720 - val_accuracy: 0.8216 - val_loss: 0.3904\n",
      "Epoch 3/5\n",
      "\u001b[1m18000/18000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 31ms/step - accuracy: 0.8501 - loss: 0.3401 - val_accuracy: 0.8204 - val_loss: 0.3964\n",
      "Epoch 4/5\n",
      "\u001b[1m18000/18000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m648s\u001b[0m 36ms/step - accuracy: 0.8669 - loss: 0.3085 - val_accuracy: 0.8189 - val_loss: 0.4064\n",
      "Epoch 5/5\n",
      "\u001b[1m18000/18000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m580s\u001b[0m 32ms/step - accuracy: 0.8820 - loss: 0.2769 - val_accuracy: 0.8133 - val_loss: 0.4278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x303b10ec0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Training the CNN model\n",
    "cnn_model.fit(X_train_pad, y_train_cat, epochs=5, batch_size=64, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94c3ea95-365d-451e-9f84-6d863f36bcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10000/10000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# 7. Making predictions on the test data\n",
    "y_pred_cnn = cnn_model.predict(X_test_pad)\n",
    "\n",
    "# Convert predictions to binary labels\n",
    "y_pred_cnn_labels = np.argmax(y_pred_cnn, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be3f24b4-e76e-4cd4-9e72-7e764bda448a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Accuracy: 81.41%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.80      0.83      0.82    160000\n",
      "    Positive       0.82      0.80      0.81    160000\n",
      "\n",
      "    accuracy                           0.81    320000\n",
      "   macro avg       0.81      0.81      0.81    320000\n",
      "weighted avg       0.81      0.81      0.81    320000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8. Evaluating the CNN model\n",
    "accuracy_cnn = accuracy_score(y_test, y_pred_cnn_labels)\n",
    "print(f\"CNN Accuracy: {accuracy_cnn * 100:.2f}%\")\n",
    "\n",
    "# Classification report for detailed metrics\n",
    "print(classification_report(y_test, y_pred_cnn_labels, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc1bb62f-0d40-4649-b0ce-b8ae4f89e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Predict sentiment for a new text\n",
    "def predict_sentiment_cnn(input_text, model, tokenizer, max_len):\n",
    "    # Clean the input text\n",
    "    clean_input_text = clean_text(input_text)\n",
    "    \n",
    "    # Convert to sequences and pad\n",
    "    input_seq = tokenizer.texts_to_sequences([clean_input_text])\n",
    "    input_pad = pad_sequences(input_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "    \n",
    "    # Predict sentiment using the CNN model\n",
    "    prediction = model.predict(input_pad)\n",
    "    \n",
    "    # Return sentiment result\n",
    "    return 'Positive' if np.argmax(prediction) == 1 else 'Negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03ced5e2-d1c1-4e0f-b77f-c42dbdb057bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "The sentiment of the given text is: Positive\n"
     ]
    }
   ],
   "source": [
    "# Example of predicting sentiment for a new text\n",
    "input_text = \"I really love this series, it's fantastic!\"\n",
    "result = predict_sentiment_cnn(input_text, cnn_model, tokenizer, max_length)\n",
    "print(f\"The sentiment of the given text is: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17221df4-7bdb-44c2-86ec-0b47e4110a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in the new Keras format\n",
    "cnn_model.save('sentiment_cnn_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c60b9ec-42cf-46db-953d-c97b0048a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# After fitting the tokenizer\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603a2b56-39c1-4e26-bc11-33182b3a49a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
